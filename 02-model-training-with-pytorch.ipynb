{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training in PyTorch\n",
        "\n",
        "In this lab, you will explore the core concepts and steps involved in ***building a convolutional neural network (CNN) using PyTorch***, a leading deep learning framework. CNNs are particularly well-suited for image classification tasks due to their ability to automatically learn spatial hierarchies in images.\n",
        "\n",
        "**Throughout the lab, you will:**\n",
        "\n",
        "* Define a simple CNN architecture using torch.nn.Module.\n",
        "* Work with image data by loading and transforming it for training.\n",
        "* Implement the forward pass with convolutional, pooling, and fully connected layers.\n",
        "* Optimize the model using an optimizer like SGD or Adam.\n",
        "* Evaluate the model's performance with metrics such as loss and accuracy.\n",
        "\n",
        "By the end of the lab, you will have a foundational understanding of CNNs in PyTorch and how to train them for image classification tasks. This hands-on experience will serve as a stepping stone for building more advanced, custom CNN architectures.\n",
        "\n",
        "As was practice in the introductory course, `XXXX` means you have to fill in the correct code. If you are following along and not in our course at the University of Rhode Island, you can find the answers in the `02-model-training-with-pytorch-ANSWERKEY.ipynb` file in the repository.\n",
        "\n",
        "Let’s begin by importing the necessary libraries for your first CNN.\n",
        "\n",
        "## 1. Set up"
      ],
      "metadata": {
        "id": "kpU_VSkO8g3H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoXxDsjobaa1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Parameters for Convolutional Layers\n",
        "\n",
        "There a few parameters that we have to specify for each convolutional layer that we add. Below is a description of what they are and how to select appropriate values.\n",
        "\n",
        "#### **`in_channels`: The Number of Input Feature Maps**\n",
        "\n",
        "* `in_channels` refers to the number of input channels (feature maps) being fed into a convolutional layer.\n",
        "\n",
        "* It must match the number of output channels from the previous layer (except for the first layer, which depends on the input data).\n",
        "\n",
        "  **First Convolutional Layer**:\n",
        "\n",
        "  * If the input is an RGB image (CIFAR-10, ImageNet, etc.), it has 3 channels (R, G, B), so `in_channels`=3.\n",
        "\n",
        "  * If the input is a grayscale image (MNIST, medical imaging, etc.), it has 1 channel, so `in_channels`=1.\n",
        "\n",
        "  **Subsequent Layers**:\n",
        "\n",
        "  * The `in_channels` for each layer is equal to the out_channels of the previous convolutional layer.\n",
        "\n",
        "#### **`out_channels`: The Number of Output Feature Maps**\n",
        "\n",
        "* `out_channels` determines how many feature maps (filters) the convolutional layer will output.\n",
        "\n",
        "* Each filter in a CNN learns to detect different features (edges, textures, shapes, etc.), so increasing `out_channels` allows the model to learn more complex patterns.\n",
        "\n",
        "  **Typical Design Choices:**\n",
        "\n",
        "  * Start with a small number of filters (e.g., `out_channels`=16 or\n",
        "`out_channels`=32) to extract low-level patterns.\n",
        "\n",
        "  * Gradually increase `out_channels` (e.g., 32 → 64 → 128 → 256) as the network goes deeper, capturing more abstract features.\n",
        "\n",
        "#### **`kernel_size`: The x,y dimensions of the filters**\n",
        "  \n",
        "  * Start with a smaller filter (e.g., `kernel_size`=3) and work your way to larger filters if needed.\n",
        "\n",
        "#### **`padding`: The additional pixels added around your image**\n",
        "  \n",
        "  * Typically set to zero.\n",
        "  * Add padding if the information at the edges of your images is highly important to your task.\n",
        "  \n",
        "#### **`stride`: The number of pixels we shift our kernel**\n",
        "\n",
        "* `stride` refers to the size of the shift of the kernel across the image at each step.\n",
        "* Practitioners oven opt for a `stride` of `1` to capture the largest amount of detail. This is the default value, so we can leave it out for now."
      ],
      "metadata": {
        "id": "e9YNNkic5M15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Parameters for Pooling Layers\n",
        "\n",
        "There a few parameters that we have to specify for each ***pooling*** layer that we add. Below is a description of what they are and how to select appropriate values.\n",
        "\n",
        "#### **`kernel_size`: The size of the pooling kernel**\n",
        "\n",
        "* `kernel_size` refers to the size of the pooling kernel that you are applying. A common value to select in practice is `2`. The reduces the size of the feature map by an order of 2.\n",
        "\n",
        "#### **`stride`: The number of pixels we shift our kernel**\n",
        "\n",
        "* `stride` refers to the size of the shift of the pooling kernel across the image at each step.\n",
        "* Practitioners oven opt for no overlap with their pooling kernel. Hence, if you select `2` for your pooling `kernel_size`, then you should select `2` for the `stride`.\n"
      ],
      "metadata": {
        "id": "5vOPVP_bJCOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Parameters for Fully Connected Layers\n",
        "\n",
        "There a few parameters that we have to specify for our ***fully connected*** layer. We must specify 4 numbers in `nn.Linear(#, #, #, #)`.\n",
        "\n",
        "* The first number is the input depth (i.e., the `output_size` of the last layer).\n",
        "* The second and third numbers are the dimensions of the image.\n",
        "* The final number is the number of classes in your dataset."
      ],
      "metadata": {
        "id": "b2_bZyvUKV2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Let's build your first custom CNN!"
      ],
      "metadata": {
        "id": "OYdTX1BHK5gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(XXXX)\n",
        "        self.conv2 = nn.Conv2d(XXXX)\n",
        "        self.pool = nn.MaxPool2d(XXXX)\n",
        "        self.fc1 = nn.Linear(XXXX)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = XXXX\n",
        "        x = XXXX\n",
        "        x = XXXX  # Hint: Flatten before passing to fully connected layer\n",
        "        x = XXXX\n",
        "        return x"
      ],
      "metadata": {
        "id": "qqO0Ju9Bei1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Prepare the Data\n",
        "\n",
        "For this lab, we will be using the MNIST dataset. This is a popular dataset containing black-and-white images of numbers."
      ],
      "metadata": {
        "id": "EdFe0STFK996"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(XXXX)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(XXXX)"
      ],
      "metadata": {
        "id": "2O5dEFQ08tA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training the CNN\n",
        "\n",
        "First we must specify a few things about our training process:\n",
        "* What model we will use,\n",
        "* what loss function we want,\n",
        "* and what optimizer we want (where we also specify our learning rate)."
      ],
      "metadata": {
        "id": "3ELW6kgALNuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = XXXX\n",
        "criterion = XXXX\n",
        "optimizer = XXXX"
      ],
      "metadata": {
        "id": "S4mNpkxm8u23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can create a loop to run through the batches of data and update the weights for each batch.\n",
        "\n",
        "We will also save the training loss and report it back once after each epoch."
      ],
      "metadata": {
        "id": "lzZbFQ_WLee8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for XXXX, XXXX in XXXX:\n",
        "        XXXX, XXXX = XXXX, XXXX\n",
        "\n",
        "        XXXX\n",
        "        XXXX\n",
        "        loss = XXXX\n",
        "        XXXX\n",
        "        XXXX\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader):.4f}\")\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "oVm9IdZH8w4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Congratulations**! You've successfully trained your first custom CNN!\n",
        "\n",
        "## 8. Exploration Quest\n",
        "\n",
        "Try changing up some of the model parameters, such as the number of filters, stride, and padding to see how they impact your training.\n",
        "\n",
        "Then, try changing some of the training parameters, such as the batch size, loss function, optimizer, and learning rate, and see how they impact the model training.\n",
        "\n",
        "Can you find an \"optimal\" set of parameters for this task?"
      ],
      "metadata": {
        "id": "VMSyj5RxLpN-"
      }
    }
  ]
}