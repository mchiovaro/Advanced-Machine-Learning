{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl5UV6f9VkIr"
      },
      "source": [
        "# **Natural Language Processing with Pytorch**\n",
        "\n",
        "This notebook will guide you through key **Natural Language Processing (NLP) concepts** using **PyTorch**, from **tokenization** to **deep learning models** like **LSTMs and Transformers**.\n",
        "\n",
        "**Key Topics Covered:**\n",
        "- **Tokenization** (using Hugging Face tokenizers)\n",
        "- **Word Embeddings** (Custom + Pretrained BERT)\n",
        "- **Building an LSTM for NLP**\n",
        "- **Using Transformers for NLP**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 1: Installs, Imports, Seed, and GPU Utilization**"
      ],
      "metadata": {
        "id": "AkS-BDSjVy3K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7xPi4RgVkIv"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, get_scheduler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sz6295_VkIw"
      },
      "source": [
        "## **Part 2: Tokenization & Vocabulary Building**\n",
        "\n",
        "Before we can process text using deep learning, we need to **convert text into numerical format**. Tokenization is the process of **splitting text into smaller units** (words, subwords, or characters). We'll use **Hugging Face's AutoTokenizer** for modern subword tokenization instead of traditional methods like NLTK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgcX0KRxVkIx"
      },
      "outputs": [],
      "source": [
        "# Load a pretrained tokenizer (BERT-based)\n",
        "tokenizer = AutoTokenizer.XXXX(\"bert-base-uncased\")\n",
        "\n",
        "# Sample dataset: A few example sentences\n",
        "corpus = [\n",
        "    \"Deep learning is amazing\",\n",
        "    \"Natural language processing is a branch of AI\",\n",
        "    \"PyTorch makes NLP easier\",\n",
        "    \"Transformers are powerful models\",\n",
        "    \"XXXX ADD SOME SENTENCES!\",\n",
        "    \"XXXX ADD SOME SENTENCES!\",\n",
        "    \"XXXX ADD SOME SENTENCES!\",\n",
        "    \"XXXX ADD SOME SENTENCES!\"\n",
        "]\n",
        "\n",
        "# Tokenize the sentences using the BERT tokenizer\n",
        "tokenized_corpus = [tokenizer.tokenize(sentence.lower()) for sentence in corpus]\n",
        "\n",
        "# Let's check how the tokenizer splits words\n",
        "print(\"\\nTokenized Corpus:\")\n",
        "print(tokenized_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGR6boD3VkIx"
      },
      "source": [
        "### Building a Vocabulary\n",
        "Once we have tokenized text, we need to assign **unique numerical indices** to each token to create a vocabulary.\n",
        "We'll create a **word2idx mapping** where each unique token gets a unique number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ydjg1c6VkIy"
      },
      "outputs": [],
      "source": [
        "def build_vocab(corpus):\n",
        "    \"\"\"\n",
        "    Builds a vocabulary dictionary from tokenized text.\n",
        "    Assigns a unique index to each token.\n",
        "    \"\"\"\n",
        "    counter = Counter()\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)\n",
        "\n",
        "    # Create a mapping of words to indices, starting from 2\n",
        "    word2idx = {word: idx+2 for idx, word in enumerate(counter.keys())}\n",
        "\n",
        "    # Add special tokens for unknown words and padding\n",
        "    word2idx[\"<unk>\"] = 0\n",
        "    word2idx[\"<pad>\"] = 1\n",
        "\n",
        "    return word2idx\n",
        "\n",
        "# Build vocabulary from tokenized corpus\n",
        "vocab = build_vocab(tokenized_corpus)\n",
        "print(\"Vocabulary size:\", len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPgjL632VkIy"
      },
      "source": [
        "## **Part 3: Word Embeddings (Custom & Pretrained BERT)**\n",
        "\n",
        "Neural networks **do not understand text** directly, so we need to **convert words into dense numerical vectors**.\n",
        "\n",
        "We'll explore two approaches:\n",
        "- **Custom Embeddings**: Learned during training (random initialization)\n",
        "- **Pretrained Embeddings**: Extracted from BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBJHtUayVkIz"
      },
      "outputs": [],
      "source": [
        "# Custom Word Embeddings (random initialization)\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBED_DIM = 50  # number of features per word\n",
        "\n",
        "embedding_layer = nn.Embedding(VOCAB_SIZE, EMBED_DIM).to(device)\n",
        "\n",
        "# Sample sentence to tensor\n",
        "sample_sentence = \"Deep learning is powerful\"\n",
        "sample_tokens = tokenizer.tokenize(sample_sentence.lower())\n",
        "sample_indices = [vocab.get(word, vocab[\"<unk>\"]) for word in sample_tokens]\n",
        "sample_tensor = torch.tensor(sample_indices).unsqueeze(0).to(device)\n",
        "\n",
        "# Get the embedding representation\n",
        "embedded_sentence = embedding_layer(sample_tensor)\n",
        "print(\"\\nCustom Embedding Shape:\", embedded_sentence.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 4: Using Pretrained Embeddings (BERT)**\n",
        "\n",
        "Instead of learning word embeddings from scratch, we can use **pretrained embeddings** from models like BERT. These embeddings capture **semantic meaning** and are **contextualized**, meaning they depend on the sentence."
      ],
      "metadata": {
        "id": "GXNh4yPtW2aF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pretrained BERT model\n",
        "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "\n",
        "# Sample sentence\n",
        "sentence = \"Deep learning is transforming AI\"\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}  # Move to GPU if available\n",
        "\n",
        "# Get embeddings from BERT\n",
        "with torch.no_grad():\n",
        "    outputs = bert_model(**inputs)\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "print(\"BERT Embedding Shape:\", last_hidden_states.shape)  # Shape: (1, seq_length, 768)"
      ],
      "metadata": {
        "id": "9TsacRXkW7Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîπ Custom vs Pretrained Embeddings: Key Differences\n",
        "| Feature            | Custom Embeddings | Pretrained (BERT) |\n",
        "|--------------------|------------------|------------------|\n",
        "| Trained on dataset | Yes              | No (already trained) |\n",
        "| Contextualized?    | No               | Yes |\n",
        "| Captures semantics | Limited          | Strong |\n",
        "| Computational cost | Low              | High |"
      ],
      "metadata": {
        "id": "U-PXEmr4W9JJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 5: Applying Transformers for NLP**\n",
        "\n",
        "### **Understanding the IMDB Dataset and the Task**\n",
        "\n",
        "The IMDB dataset is a widely used benchmark dataset for sentiment analysis. It consists of 50,000 movie reviews from IMDB, labeled as either positive (1) or negative (0). The dataset is divided into:\n",
        "* 25,000 training reviews\n",
        "* 25,000 test reviews\n",
        "\n",
        "Each set contains an equal number of positive and negative reviews. Here are two examples:\n",
        "* ‚úÖ Positive Review: \"This movie was absolutely fantastic! The storyline was engaging, and the performances were top-notch.\"\n",
        "* ‚ùå Negative Review: \"Terrible film. The acting was bad, the plot was predictable, and I regret watching it.\"\n",
        "\n",
        "The goal is to build a binary classification model that takes a movie review as input and predicts whether it is positive (1) or negative (0).\n",
        "\n",
        "**Why is this useful?**\n",
        "\n",
        "* Companies use sentiment analysis to analyze customer feedback.\n",
        "* It helps in opinion mining from social media, news, and reviews.\n",
        "* Used in automated content moderation (e.g., filtering harmful content).\n",
        "\n",
        "**Challenges in Text-Based Sentiment Analysis**\n",
        "\n",
        "Unlike structured numerical data, text data presents unique challenges:\n",
        "* Different sentence structures:\n",
        "  * \"I didn't like this movie.\" (Negative)\n",
        "  * \"I didn't expect to like this movie, but I did.\" (Positive)\n",
        "* Sarcasm & Negation Handling:\n",
        "  * \"Oh great, another terrible remake.\" (Negative)\n",
        "* Word Importance:\n",
        "  * Some words (e.g., \"not\") can flip the sentiment of a sentence.\n",
        "* Large Vocabulary:\n",
        "  * Many unique words ‚Üí Requires efficient embedding techniques.\n",
        "\n",
        "**We Will Solve This Using BERT**\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers) is a pretrained deep learning model that:\n",
        "* Understands context in text (unlike older methods like TF-IDF)\n",
        "* Is pretrained on vast amounts of text data\n",
        "* Handles long-range dependencies in text\n",
        "\n",
        "We will fine-tune BERT to classify IMDB reviews as positive or negative using PyTorch and Hugging Face‚Äôs transformers library."
      ],
      "metadata": {
        "id": "sB9jhk88XbuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 1: Load the Data"
      ],
      "metadata": {
        "id": "vLVu-s5yiRem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the IMDB dataset (already pre-split into train and test sets)\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Reduce dataset size to save memory (use only 500 training samples)\n",
        "train_texts = dataset['train']['text'][:500]\n",
        "train_labels = dataset['train']['label'][:500]\n",
        "\n",
        "# Use full test set for evaluation\n",
        "test_texts = dataset['test']['text'][:100]\n",
        "test_labels = dataset['test']['label'][:100]\n",
        "\n",
        "# Print dataset size\n",
        "print(f\"Training samples: {len(train_texts)}\")\n",
        "print(f\"Testing samples: {len(test_texts)}\")"
      ],
      "metadata": {
        "id": "kCS2Qgh2iaMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 2: Tokenize the Data"
      ],
      "metadata": {
        "id": "yC7BJDYeibBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer for pretrained DistilBERT\n",
        "tokenizer = AutoTokenizer.XXXX(\"distilbert-base-uncased\")\n",
        "\n",
        "# Function to tokenize text\n",
        "def tokenize_function(texts):\n",
        "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
        "\n",
        "# Tokenize training and testing texts\n",
        "train_encodings = XXXX(XXXX)\n",
        "test_encodings = XXXX(XXXX)\n",
        "\n",
        "# Convert labels into PyTorch tensors\n",
        "train_labels_tensor = XXXX.XXXX(XXXX)\n",
        "test_labels_tensor = XXXX.XXXX(XXXX)"
      ],
      "metadata": {
        "id": "gUwBlrYHifyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 3: Create DataSet Class"
      ],
      "metadata": {
        "id": "YJ3rB7vsigNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset class to handle tokenized data\n",
        "class IMDBDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings  # Tokenized text\n",
        "        self.labels = labels  # Corresponding labels (0 = negative, 1 = positive)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)  # Number of samples in dataset\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieve tokenized text and label for a given index\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "# Create dataset objects for training and testing\n",
        "train_dataset = XXXX(XXXX, XXXX)\n",
        "test_dataset = XXXX(XXXX, XXXX)"
      ],
      "metadata": {
        "id": "7PPHi1MXimT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 4: Create DataLoaders"
      ],
      "metadata": {
        "id": "5q345cbPimwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = XXXX(XXXX, XXXX, XXXX)\n",
        "test_loader = XXXX(XXXX, XXXX, XXXX)"
      ],
      "metadata": {
        "id": "03sAK0x4iq_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 5: Load Pretrained DistilBERT Model"
      ],
      "metadata": {
        "id": "CKpnHYxkiwJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained DistilBERT model for binary classification\n",
        "model = AutoModelForSequenceClassification.XXXX(\"distilbert-base-uncased\", num_labels=2)\n",
        "\n",
        "# Move model to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "XXXX"
      ],
      "metadata": {
        "id": "RaEkUEvJi3LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 7: Define Optimizer & Loss Function"
      ],
      "metadata": {
        "id": "HhYqm8gTi6GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = XXXX\n",
        "criterion = XXXX\n",
        "\n",
        "# Calculate the number of training steps (total batches in 2 epochs)\n",
        "num_training_steps = len(train_loader) * 2\n",
        "\n",
        "# Create a learning rate scheduler (linearly decreases learning rate)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\", optimizer=XXXX, num_warmup_steps=0, num_training_steps=XXXX\n",
        ")"
      ],
      "metadata": {
        "id": "5tgcHdI9jA_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 8: Training Loop"
      ],
      "metadata": {
        "id": "yHU9KOmgjDw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time  # Add timing to monitor training speed\n",
        "\n",
        "# Enable mixed precision training to reduce memory usage\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "num_epochs = 1  # Reduce to 1 epoch for faster training\n",
        "start_time = time.time()  # Start timer\n",
        "\n",
        "XXXX  # Set model to training mode\n",
        "\n",
        "for XXXX in XXXX:\n",
        "    print(f\"Starting epoch {epoch+1}...\")\n",
        "\n",
        "    for batch_idx, XXXX in enumerate(XXXX):\n",
        "        XXXX  # Reset gradients\n",
        "\n",
        "        batch = {key: val.to(device) for key, val in batch.items()}  # Move to device\n",
        "\n",
        "        with torch.cuda.amp.autocast():  # Mixed precision\n",
        "            outputs = model(**batch)  # Forward pass\n",
        "            loss = outputs.loss  # Compute loss\n",
        "\n",
        "        # Backpropagation with mixed precision scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # Print loss every 10 batches\n",
        "        if batch_idx % 10 == 0:\n",
        "          print(f\"Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # If training takes too long (> 10 minutes), break\n",
        "        if time.time() - start_time > 600:\n",
        "            print(\"Training took too long, stopping early.\")\n",
        "            break\n",
        "\n",
        "    print(f\"Epoch {epoch+1} completed. Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"Training finished.\")"
      ],
      "metadata": {
        "id": "iz2i0whnjHfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 9: Evaluation"
      ],
      "metadata": {
        "id": "bg6mutR9iK0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model to evaluation mode (disables dropout and gradients)\n",
        "model.eval()\n",
        "\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "# Disable gradient computation to save memory during testing\n",
        "with XXXX:\n",
        "    for XXXX in XXXX:\n",
        "        batch = {key: val.to(device) for key, val in batch.items()}  # Move batch to device\n",
        "        outputs = model(**batch)  # Forward pass\n",
        "        logits = outputs.logits  # Get model outputs\n",
        "\n",
        "        # Convert logits to class predictions (0 or 1)\n",
        "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        labels = batch[\"labels\"].cpu().numpy()\n",
        "\n",
        "        # Store predictions and actual labels for accuracy calculation\n",
        "        predictions.extend(preds)\n",
        "        true_labels.extend(labels)\n",
        "\n",
        "# Compute accuracy score\n",
        "accuracy = XXXX(XXXX, XXXX)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "yo6uAcBViHaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 7: Challenge Time!**\n",
        "\n",
        "**Try modifying the training setup!**\n",
        "- Change the learning rate and optimizer\n",
        "- Use a different model (e.g., 'distilbert-base-uncased')\n",
        "- Train for more epochs\n",
        "- Add regularization like dropout\n",
        "- Experiment with different batch sizes\n",
        "- Visualize loss over training steps"
      ],
      "metadata": {
        "id": "3eoMGWw9bWR0"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}