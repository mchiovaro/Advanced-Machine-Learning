{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN Comparison of Different Quantization Techniques**\n",
        "\n",
        "In this workshop we will walk through how to implement various quantization modes to a CNN model."
      ],
      "metadata": {
        "id": "SGi7H95F9Gvi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSuAVUDHdZ4d",
        "outputId": "70cf1189-a428-4c67-d2f8-a2eafcfce468"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c791c272ff0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.quantization as quant\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "torch.manual_seed(24)  # For reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Measuring Inference Time**\n",
        "\n",
        "Function that tests testing time for each quantization method"
      ],
      "metadata": {
        "id": "3_VS0fA2ZvZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_inference_time(model, dataloader, device='cpu'):\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  start_time=time.time()\n",
        "  with torch.no_grad():\n",
        "    for inputs, _ in dataloader:\n",
        "      inputs = inputs.to(device)\n",
        "      outputs = model(inputs)\n",
        "    return time.time() - start_time"
      ],
      "metadata": {
        "id": "er2DgzDWjkk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Measuring Test Accuracy**\n",
        "\n",
        "Defining a function to test accuracy of each altered model"
      ],
      "metadata": {
        "id": "R21v3CWOZ6NH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test accuracy before and after quantization\n",
        "\n",
        "def test_model(model):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "  return correct / total"
      ],
      "metadata": {
        "id": "2_LqVE8yqb1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Original CNN Class**"
      ],
      "metadata": {
        "id": "AQtb5UJVaDfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define CNN model\n",
        "class CustomCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CustomCNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.fc1 = nn.Linear(32 * 7 * 7, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(self.relu(self.conv1(x)))\n",
        "    x = self.pool(self.relu(self.conv2(x)))\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc1(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ALj8G3A8g_RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading MNIST Dataset**"
      ],
      "metadata": {
        "id": "S6NGb449aGoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load MNIST dataset/dataloader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,),(0.3081))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=False)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Qsg4PWzDgd5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cd9130a-9e94-4fd7-ac56-cfd3bb7d680b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 54.9MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.76MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.6MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.83MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparing Original Model**"
      ],
      "metadata": {
        "id": "NNoSOkmCaMuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize model, loss function, optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CustomCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "5PjbbxXoj9WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "epochs = 10\n",
        "\n",
        "print(\"Training model...\")\n",
        "for epoch in range(epochs):\n",
        "  running_loss = 0.0\n",
        "  for inputs, labels in trainloader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader):.4f}\")\n",
        "\n",
        "print(\"Training completed\")\n",
        "torch.save(model.state_dict(), \"mnist_cnn_workshop.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2R7qS-QkPET",
        "outputId": "47620552-8c0f-44e8-9b4c-dfc42562a51a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n",
            "Epoch 1, Loss: 0.1448\n",
            "Epoch 2, Loss: 0.0534\n",
            "Epoch 3, Loss: 0.0385\n",
            "Epoch 4, Loss: 0.0291\n",
            "Epoch 5, Loss: 0.0229\n",
            "Epoch 6, Loss: 0.0180\n",
            "Epoch 7, Loss: 0.0149\n",
            "Epoch 8, Loss: 0.0118\n",
            "Epoch 9, Loss: 0.0105\n",
            "Epoch 10, Loss: 0.0090\n",
            "Training completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN with Post Training Quantization (PTQ)**\n",
        "Post Training Quantization (PTQ) involves calibrating the model by passing batches of sample data through it to obtain activation distributions. These distributions are used to determine scaling factors for weights and activations."
      ],
      "metadata": {
        "id": "dr5OHWVCgrSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load original model weights\n",
        "\n",
        "model = CustomCNN()\n",
        "model.load_state_dict(torch.load(\"mnist_cnn_workshop.pth\", map_location=torch.device(\"cpu\")))\n",
        "model.eval()\n",
        "\n",
        "# apply dynamic PTQ\n",
        "quantized_model_PTDQ = quant.quantize_dynamic(\n",
        "    model, {nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "#time_PTDQ = measure_inference_time(quantized_model_PTDQ, testloader,device='cpu')\n",
        "\n",
        "torch.save(quantized_model_PTDQ.state_dict(), \"mnist_cnn_quantizedPTDQ_workshop.pth\")\n",
        "print(\"Dynamic Quantization completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0hSA_EwpNPV",
        "outputId": "fe6cd2c2-11db-46db-e72b-0990dfeea561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dynamic Quantization completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load original model weights\n",
        "model = CustomCNN()\n",
        "model.load_state_dict(torch.load(\"mnist_cnn_workshop.pth\", map_location=torch.device(\"cpu\")))\n",
        "model.eval()\n",
        "\n",
        "# prepare the model for static PTQ\n",
        "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "torch.quantization.prepare(model, inplace=True)\n",
        "\n",
        "# calibrate the model with a few batches\n",
        "num_calibration_batches = 10  # adjust as needed\n",
        "for inputs, _ in iter(testloader): # use a subset of the test data for calibration\n",
        "    if num_calibration_batches == 0:\n",
        "      break\n",
        "    model(inputs)\n",
        "    num_calibration_batches -= 1\n",
        "\n",
        "# convert the model to quantized form\n",
        "quantized_model_PTSQ = torch.quantization.convert(model, inplace=True)\n",
        "\n",
        "# Measure inference time of the statically quantized model\n",
        "#time_PTSQ = measure_inference_time(quantized_model_PTSQ, testloader, device='cuda') # BACKEND SUPPORT MISSING\n",
        "\n",
        "# Save the statically quantized model\n",
        "torch.save(quantized_model_PTSQ.state_dict(), \"mnist_cnn_quantizedPTSQ_workshop.pth\")\n",
        "\n",
        "print(\"Static Quantization completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLyfJt_elIWz",
        "outputId": "a2c73b10-1cf1-4ade-9b4b-05c7a1f90e15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Static Quantization completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QAT**"
      ],
      "metadata": {
        "id": "7BNc-5hImh7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QAT_CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(QAT_CNN, self).__init__()\n",
        "    self.quant = quant.QuantStub() # convert tensors from floating point to quantized\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.fc1 = nn.Linear(32 * 7 * 7, 10)\n",
        "    self.dequant = quant.DeQuantStub() # convert tensors from quantized to floating point\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.quant(x) # convert input to INT8\n",
        "    x = self.pool(self.relu(self.conv1(x)))\n",
        "    x = self.pool(self.relu(self.conv2(x)))\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.dequant(x) # convert back to FP32\n",
        "    return x"
      ],
      "metadata": {
        "id": "MpIygGHBvhE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load original model weights\n",
        "model = QAT_CNN()\n",
        "model.qconfig = quant.get_default_qat_qconfig(\"fbgemm\")\n",
        "quant.prepare_qat(model, inplace=True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "print(\"Starting QAT Training...\")\n",
        "\n",
        "# train\n",
        "for epoch in range(epochs):\n",
        "  for inputs, labels in trainloader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
        "print(\"QAT Training completed\")\n",
        "\n",
        "# save\n",
        "model.eval()\n",
        "quantized_model_QAT = quant.convert(model) # convert to INT8\n",
        "torch.save(quantized_model_QAT.state_dict(), \"mnist_cnn_quantizedQAT_workshop.pth\")\n",
        "print(\"QAT model saved\")"
      ],
      "metadata": {
        "id": "EutfeHIVmyNZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33189081-a60f-4707-e16a-679e3eafadc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 0.0302\n",
            "Epoch 2: Loss = 0.0039\n",
            "Epoch 3: Loss = 0.0014\n",
            "Epoch 4: Loss = 0.0006\n",
            "Epoch 5: Loss = 0.0011\n",
            "Epoch 6: Loss = 0.0000\n",
            "Epoch 7: Loss = 0.0000\n",
            "Epoch 8: Loss = 0.0001\n",
            "Epoch 9: Loss = 0.0001\n",
            "Epoch 10: Loss = 0.0001\n",
            "QAT Training completed\n",
            "QAT model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# check model file sizes\n",
        "original_size = os.path.getsize(\"mnist_cnn_workshop.pth\") / 1024 # KB\n",
        "quantized_size_PTSQ = os.path.getsize(\"mnist_cnn_quantizedPTSQ_workshop.pth\") / 1024 # KB\n",
        "quantized_size_PTDQ = os.path.getsize(\"mnist_cnn_quantizedPTDQ_workshop.pth\") / 1024 # KB\n",
        "quantized_size_QAT = os.path.getsize(\"mnist_cnn_quantizedQAT_workshop.pth\") / 1024 # KB\n",
        "\n",
        "print(f\"Original model size: {original_size:.2f} KB\")\n",
        "print(f\"Quantized model size (PTSQ): {quantized_size_PTSQ:.2f} KB\")\n",
        "print(f\"Compression Ratio (PTSQ): {original_size / quantized_size_PTSQ:.2f}x\")\n",
        "print(f\"Quantized model size (PTDQ): {quantized_size_PTDQ:.2f} KB\")\n",
        "print(f\"Compression Ratio (PTDQ): {original_size / quantized_size_PTDQ:.2f}x\")\n",
        "print(f\"Quantized model size (QAT): {quantized_size_QAT:.2f} KB\")\n",
        "print(f\"Compression Ratio (QAT): {original_size / quantized_size_QAT:.2f}x\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yIx6i9Wp8iv",
        "outputId": "9e31b8a0-8f98-4f2d-c766-f3a0f574676c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model size: 82.74 KB\n",
            "Quantized model size (PTSQ): 27.44 KB\n",
            "Compression Ratio (PTSQ): 3.01x\n",
            "Quantized model size (PTDQ): 37.86 KB\n",
            "Compression Ratio (PTDQ): 2.19x\n",
            "Quantized model size (QAT): 27.96 KB\n",
            "Compression Ratio (QAT): 2.96x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test accuracies before and after quantization\n",
        "original_model = CustomCNN()\n",
        "original_model.load_state_dict(torch.load(\"mnist_cnn_workshop.pth\", map_location=torch.device(\"cpu\")))\n",
        "original_accuracy = test_model(model)\n",
        "\n",
        "quantized_model_PTSQ = CustomCNN()\n",
        "quantized_model_PTSQ.load_state_dict(torch.load(\"mnist_cnn_quantizedPTSQ_workshop.pth\", map_location=torch.device(\"cpu\")), strict=False)\n",
        "quantized_model_PTSQ.eval()\n",
        "quantized_model_PTSQ.to('cpu')\n",
        "\n",
        "quantized_model_PTDQ = CustomCNN()\n",
        "quantized_model_PTDQ.load_state_dict(torch.load(\"mnist_cnn_quantizedPTDQ_workshop.pth\", map_location=torch.device(\"cpu\")), strict=False)\n",
        "quantized_model_PTDQ.eval()\n",
        "quantized_model_PTDQ.to('cpu')\n",
        "\n",
        "quantized_model_QAT = QAT_CNN()\n",
        "quantized_model_QAT.load_state_dict(torch.load(\"mnist_cnn_quantizedQAT_workshop.pth\", map_location=torch.device(\"cpu\")))\n",
        "quantized_model_QAT.eval()\n",
        "quantized_model_QAT.to('cpu')\n",
        "\n",
        "\n",
        "quantized_accuracy_PTSQ = test_model(quantized_model_PTSQ)\n",
        "\n",
        "quantized_accuracy_PTDQ = test_model(quantized_model_PTDQ)\n",
        "\n",
        "quantized_accuracy_QAT = test_model(quantized_model_QAT)\n",
        "\n",
        "print(f\"Original Model Accuracy: {original_accuracy * 100:.2f}%\")\n",
        "print(f\"Quantized Model Accuracy (PTSQ): {quantized_accuracy_PTSQ * 100:.2f}%\")\n",
        "print(f\"Quantized Model Accuracy (PTDQ): {quantized_accuracy_PTDQ * 100:.2f}%\")\n",
        "print(f\"Quantized Model Accuracy (QAT): {quantized_accuracy_QAT * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "w09XH3wKrLtI",
        "outputId": "90e75fdf-5310-4c9a-9501-181a8cee1210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for CustomCNN:\n\tWhile copying the parameter named \"conv1.weight\", whose dimensions in the model are torch.Size([16, 1, 3, 3]) and whose dimensions in the checkpoint are torch.Size([16, 1, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"conv2.weight\", whose dimensions in the model are torch.Size([32, 16, 3, 3]) and whose dimensions in the checkpoint are torch.Size([32, 16, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-aa801ee984cf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mquantized_model_PTSQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mquantized_model_PTSQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mnist_cnn_quantizedPTSQ_workshop.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mquantized_model_PTSQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mquantized_model_PTSQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CustomCNN:\n\tWhile copying the parameter named \"conv1.weight\", whose dimensions in the model are torch.Size([16, 1, 3, 3]) and whose dimensions in the checkpoint are torch.Size([16, 1, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"conv2.weight\", whose dimensions in the model are torch.Size([32, 16, 3, 3]) and whose dimensions in the checkpoint are torch.Size([32, 16, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Dynamic Quantization Inference Time: {time_PTDQ:.4f} seconds\")\n",
        "print(f\"Static Quantization Inference Time: {time_PTSQ:.4f} seconds\")\n",
        "print(f\"Quantization Aware Training Inference Time: {time_QAT:.4f} seconds\")"
      ],
      "metadata": {
        "id": "r8Mp4fBc5RJn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}