{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRqeUSBmjmiY"
      },
      "source": [
        "# Hosted MLFlow Alternatives\n",
        "\n",
        "There are a few problems with using MLFlow, but mainly, the issue of where to host the MLFlow server arises. Sure you may be able to start an instance within colab, however, when your virtual environment is killed, your data is irrecoverable.\n",
        "\n",
        "## Solutions\n",
        "\n",
        "There are two main solutions to dealing with this problem, namely:\n",
        "\n",
        "1. **Weights & Biases** (wandb)\n",
        "2. **Neptune.ai** (neptune)\n",
        "\n",
        "Both of these packages / platforms share a very common API (same methods) with MLFlow. Though we will be covering basic usage of each platform, we reccomend starting with **Weights & Biases** (the platform is more intuitive, and offers many more features).\n",
        "\n",
        "## Creating an Account\n",
        "\n",
        "These platforms are **free** for students and educators. Though we will be using specifically wandb in this notebook, the APIs are very similar.\n",
        "\n",
        "Create your accounts here:\n",
        "1. [https://wandb.ai/](https://wandb.ai/)\n",
        "2. [https://neptune.ai/](https://neptune.ai/)\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "In order to use either of these platforms, you must first start a `run`. What is a `run`? The name is fairly intuitive -- a run is simply a run of your model training process.\n",
        "\n",
        "### Weights & Biases (wandb)\n",
        "\n",
        "First, install wandb if you haven't already:\n",
        "\n",
        "> ```\n",
        "> pip install wandb\n",
        "> ```\n",
        "\n",
        "#### Initialize a new run\n",
        "> ```python\n",
        "> import wandb\n",
        "> run = wandb.init(project=\"your_project_name\")\n",
        "> ```\n",
        "\n",
        "#### Log hyperparameters\n",
        "\n",
        "> ```python\n",
        "> config = { \"epochs\": 10 }\n",
        "> run = wandb.init(..., config=config)\n",
        "> # - or -\n",
        "> run.config.update({\"epochs\": 10, ...})\n",
        "> # - or -\n",
        "> run.config[\"epochs\"] = 10\n",
        "\n",
        "\n",
        "#### Log metrics\n",
        "> ```python\n",
        "> wandb.log({\"epoch\": epoch, \"loss\": loss, \"accuracy\": accuracy})\n",
        "> ```\n",
        "\n",
        "#### Finish Run\n",
        "\n",
        "> ```python\n",
        "> run.finish()\n",
        "> ```\n",
        "\n",
        "\n",
        "### Neptune.ai (neptune)\n",
        "\n",
        "First, install neptune if you haven't already:\n",
        "\n",
        "> `pip install neptune`\n",
        "\n",
        "#### Initialize a new run\n",
        "\n",
        "> ```python\n",
        "> import neptune\n",
        "> run = neptune.init_run(project=\"your_workspace/your_project\")\n",
        "> ```\n",
        "\n",
        "#### Log hyperparameters\n",
        "\n",
        "> ```python\n",
        "> run[\"parameters\"] = { \"epochs\": 10 }\n",
        "> # - or -\n",
        "> run[\"parameters/epochs\"] = 10\n",
        "> ```\n",
        "\n",
        "#### Log metrics\n",
        "\n",
        "> ```python\n",
        "> run[f\"metrics/epoch_{epoch}/loss\"] = loss\n",
        "> run[f\"metrics/epoch_{epoch}/accuracy\"] = accuracy\n",
        "> ```\n",
        "\n",
        "### Finish Run\n",
        "\n",
        "> ```python\n",
        "> run.stop()\n",
        "> ```\n",
        "\n",
        "\n",
        "\n",
        "## Some Additional Information\n",
        "\n",
        "### **Use the config as a source of truth**\n",
        "\n",
        "Example:\n",
        "\n",
        "**Do NOT do the following**\n",
        "\n",
        "```python\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    ...\n",
        "```\n",
        "\n",
        "**Instead, do:**\n",
        "```python\n",
        "for epoch in range(config.epochs):\n",
        "    ...\n",
        "```\n",
        "\n",
        "### Advantages of these platforms\n",
        "\n",
        "1. You can save models throughout the training process (we will be doing this)\n",
        "2. You can fork runs by importing an existing model and overwriting the data. (e.g. adjust learning_rate around epoch 40)\n",
        "3. Statistics about CPU and GPU usage are automatically recorded.\n",
        "\n",
        "## Key Differences\n",
        "**between MLFlow, WandB and Neptune.ai**\n",
        "\n",
        "1. WandB and MLFlow share a very very similar API design (e.g. both use `run.log`), whereas Neptune has a different way of doing this.\n",
        "2. WandB has an artifact registry\n",
        "3. WandB and Neptune are hosted for you (in the cloud)\n",
        "4. Only MLFlow is open-source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLSJd95ar0Vk"
      },
      "outputs": [],
      "source": [
        "%pip install -q wandb\n",
        "\n",
        "# - or\n",
        "# %pip install -q neptune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxM38S4hr7lj"
      },
      "source": [
        "# Hotdog, or Not hotdog\n",
        "\n",
        "<img src=\"https://www.oreilly.com/content/wp-content/uploads/sites/2/2020/01/Figure_1-71076f8ac360d6a065cf19c6923310d2.jpg\" width=\"300\"/>\n",
        "\n",
        "Many of you may have seen, or heard of the show, Silicon Valley. One of the famous clips from this show was when someone made a ML algorithm which was able to predict if something was, or was not a Hotdog.\n",
        "\n",
        "As an example, let's implement this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vqm5X7k-tPRR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEO1SjISvJmk"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset that we will be using already segmented our images to train and test. (How convenient!)\n",
        "\n",
        "https://www.kaggle.com/datasets/dansbecker/hot-dog-not-hot-dog/data\n",
        "\n",
        "Let's get started by defining our dataloader and dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nphc7iG7wfVV"
      },
      "outputs": [],
      "source": [
        "# we need to download this dataset from github (this is on kaggle\n",
        "# but it is a longer process to download from kaggle; need api token)\n",
        "!git clone https://github.com/youngsoul/hotdog-not-hotdog-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZLSfgnxr6MI"
      },
      "outputs": [],
      "source": [
        "class HotdogDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Load image paths and labels\n",
        "        for label, sub_dir in enumerate(['hot_dog', 'not_hot_dog']):\n",
        "            sub_dir_path = os.path.join(root_dir, sub_dir)\n",
        "            for filename in os.listdir(sub_dir_path):\n",
        "                if filename.endswith(\".jpg\"):\n",
        "                    self.image_paths.append(os.path.join(sub_dir_path, filename))\n",
        "                    self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Loads and returns a sample (image and label).\"\"\"\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thxwRi0dy7DS"
      },
      "source": [
        "### Define our Transforms & Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeGz968wy15C"
      },
      "outputs": [],
      "source": [
        "# the transforms that we are using\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resize to a fixed size\n",
        "    transforms.ToTensor(),  # Convert image to a tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
        "])\n",
        "\n",
        "train_dataset = HotdogDataset(\"./hotdog-not-hotdog-dataset/train\", transform=transform)\n",
        "test_dataset = HotdogDataset(\"./hotdog-not-hotdog-dataset/test\", transform=transform)\n",
        "val_dataset = HotdogDataset(\"./hotdog-not-hotdog-dataset/holdout\", transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge7zX-HDzA72"
      },
      "source": [
        "### Create our Run\n",
        "\n",
        "For now, you can just follow the instructions to login, however, if you are interested in secrets management, see the following:\n",
        "\n",
        "```python\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "    from google.colab import userdata\n",
        "    key = userdata.get('wandb_api_key')\n",
        "elif 'KAGGLE_CONTAINER_NAME' in os.environ:\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "    user_secrets = UserSecretsClient()\n",
        "    key = user_secrets.get_secret(\"wandb_api_key\")\n",
        "else:\n",
        "    key = None\n",
        "\n",
        "wandb.login(key=key)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL4TezRpzB0A"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# define our run\n",
        "run = wandb.init(\n",
        "  project=\"hotdog-not-hotdog\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p8-0TxszggM"
      },
      "source": [
        "### Define our Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icVvVP4ozimT"
      },
      "outputs": [],
      "source": [
        "# define our configuration\n",
        "# feel free to change this\n",
        "run.config.update({\n",
        "    \"n_epochs\": 10,\n",
        "    \"lr\": 0.001,\n",
        "    \"batch_size\": 64,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFT7C-e9y9ap"
      },
      "source": [
        "### Create Dataloaders\n",
        "\n",
        "Create loaders for our three different datasets: train, test and holdout (validation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyZXi16Gy1z-"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=run.config[\"batch_size\"], shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=run.config[\"batch_size\"], shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=run.config[\"batch_size\"], shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaZD447J0kYv"
      },
      "source": [
        "### Create our Model\n",
        "\n",
        "We aren't really creating a model, just fine tuning one. This will save us some time ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PraMUEs0dxd"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\n",
        "    \"cuda\" if torch.cuda.is_available() else          # for GPUs\n",
        "    \"mps\" if torch.backends.mps.is_available() else   # for Apple Silicon chips\n",
        "    \"cpu\"                                             # else\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4hKFasm1A3C"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet50\n",
        "\n",
        "# define our model, optimizer, and criterion\n",
        "model = resnet50(weights=\"IMAGENET1K_V2\").to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=run.config[\"lr\"])\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# todo: log this information into the run configuration"
      ],
      "metadata": {
        "id": "lXCARo8jCgDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuXd4m9m3UDq"
      },
      "source": [
        "### Prepare for Stats\n",
        "\n",
        "Open the dashboard (either in another tab, or here!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2W0lnsM3XLm"
      },
      "outputs": [],
      "source": [
        "# you can only see this if you are logged in.\n",
        "%wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm9ZPB9l1DWh"
      },
      "source": [
        "### Train our Model\n",
        "\n",
        "Let's create our training loop..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxOzPDMS1GEJ"
      },
      "outputs": [],
      "source": [
        "for epoch in range(run.config[\"n_epochs\"]):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    running_loss = 0\n",
        "\n",
        "    model.train()  # set train model\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # fwd pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # backwards pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # tracking\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # calculate more stats\n",
        "    avg_loss = running_loss / len(train_dataset)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    model.eval()\n",
        "    total_val = 0\n",
        "    correct_val = 0\n",
        "    running_loss_val = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # tracking\n",
        "            running_loss_val += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss_val = running_loss_val / len(val_loader)\n",
        "    accuracy_val = correct_val / total_val\n",
        "\n",
        "    # todo: add more statistics here\n",
        "    run.log({\n",
        "        \"epoch\": epoch + 1,                                       # general tracking\n",
        "        \"loss\": avg_loss, \"accuracy\": accuracy,                   # training loop\n",
        "        \"val_loss\": avg_loss_val, \"val_accuracy\": accuracy_val    # val loop\n",
        "    })"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
