{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRqeUSBmjmiY"
      },
      "source": [
        "# Hosted MLFlow Alternatives\n",
        "\n",
        "There are a few problems with using MLFlow, but mainly, the issue of where to host the MLFlow server arises. Sure you may be able to start an instance within colab, however, when your virtual environment is killed, your data is irrecoverable.\n",
        "\n",
        "## Solutions\n",
        "\n",
        "There are two main solutions to dealing with this problem, namely:\n",
        "\n",
        "1. **Weights & Biases** (wandb)\n",
        "2. **Neptune.ai** (neptune)\n",
        "\n",
        "Both of these packages / platforms share a very common API (same methods) with MLFlow. Though we will be covering basic usage of each platform, we reccomend starting with **Weights & Biases** (the platform is more intuitive, and offers many more features).\n",
        "\n",
        "## Creating an Account\n",
        "\n",
        "These platforms are **free** for students and educators. Though we will be using specifically wandb in this notebook, the APIs are very similar.\n",
        "\n",
        "Create your accounts here:\n",
        "1. [https://wandb.ai/](https://wandb.ai/)\n",
        "2. [https://neptune.ai/](https://neptune.ai/)\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "In order to use either of these platforms, you must first start a `run`. What is a `run`? The name is fairly intuitive -- a run is simply a run of your model training process.\n",
        "\n",
        "### Weights & Biases (wandb)\n",
        "\n",
        "First, install wandb if you haven't already:\n",
        "\n",
        "> ```\n",
        "> pip install wandb\n",
        "> ```\n",
        "\n",
        "#### Initialize a new run\n",
        "> ```python\n",
        "> import wandb\n",
        "> run = wandb.init(project=\"your_project_name\")\n",
        "> ```\n",
        "\n",
        "#### Log hyperparameters\n",
        "\n",
        "> ```python\n",
        "> config = { \"epochs\": 10 }\n",
        "> run = wandb.init(..., config=config)\n",
        "> # - or -\n",
        "> run.config.update({\"epochs\": 10, ...})\n",
        "> # - or -\n",
        "> run.config[\"epochs\"] = 10\n",
        "\n",
        "\n",
        "#### Log metrics\n",
        "> ```python\n",
        "> wandb.log({\"epoch\": epoch, \"loss\": loss, \"accuracy\": accuracy})\n",
        "> ```\n",
        "\n",
        "#### Finish Run\n",
        "\n",
        "> ```python\n",
        "> run.finish()\n",
        "> ```\n",
        "\n",
        "\n",
        "### Neptune.ai (neptune)\n",
        "\n",
        "First, install neptune if you haven't already:\n",
        "\n",
        "> `pip install neptune`\n",
        "\n",
        "#### Initialize a new run\n",
        "\n",
        "> ```python\n",
        "> import neptune\n",
        "> run = neptune.init_run(project=\"your_workspace/your_project\")\n",
        "> ```\n",
        "\n",
        "#### Log hyperparameters\n",
        "\n",
        "> ```python\n",
        "> run[\"parameters\"] = { \"epochs\": 10 }\n",
        "> # - or -\n",
        "> run[\"parameters/epochs\"] = 10\n",
        "> ```\n",
        "\n",
        "#### Log metrics\n",
        "\n",
        "> ```python\n",
        "> run[f\"metrics/epoch_{epoch}/loss\"] = loss\n",
        "> run[f\"metrics/epoch_{epoch}/accuracy\"] = accuracy\n",
        "> ```\n",
        "\n",
        "### Finish Run\n",
        "\n",
        "> ```python\n",
        "> run.stop()\n",
        "> ```\n",
        "\n",
        "\n",
        "\n",
        "## Some Additional Information\n",
        "\n",
        "### **Use the config as a source of truth**\n",
        "\n",
        "Example:\n",
        "\n",
        "**Do NOT do the following**\n",
        "\n",
        "```python\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    ...\n",
        "```\n",
        "\n",
        "**Instead, do:**\n",
        "```python\n",
        "for epoch in range(config.epochs):\n",
        "    ...\n",
        "```\n",
        "\n",
        "### Advantages of these platforms\n",
        "\n",
        "1. You can save models throughout the training process (we will be doing this)\n",
        "2. You can fork runs by importing an existing model and overwriting the data. (e.g. adjust learning_rate around epoch 40)\n",
        "3. Statistics about CPU and GPU usage are automatically recorded.\n",
        "\n",
        "## Key Differences\n",
        "**between MLFlow, WandB and Neptune.ai**\n",
        "\n",
        "1. WandB and MLFlow share a very very similar API design (e.g. both use `run.log`), whereas Neptune has a different way of doing this.\n",
        "2. WandB has an artifact registry\n",
        "3. WandB and Neptune are hosted for you (in the cloud)\n",
        "4. Only MLFlow is open-source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLSJd95ar0Vk"
      },
      "outputs": [],
      "source": [
        "%pip install -q wandb\n",
        "\n",
        "# - or\n",
        "# %pip install -q neptune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxM38S4hr7lj"
      },
      "source": [
        "# Hotdog, or Not hotdog\n",
        "\n",
        "<img src=\"https://www.oreilly.com/content/wp-content/uploads/sites/2/2020/01/Figure_1-71076f8ac360d6a065cf19c6923310d2.jpg\" width=\"300\"/>\n",
        "\n",
        "Many of you may have seen, or heard of the show, Silicon Valley. One of the famous clips from this show was when someone made a ML algorithm which was able to predict if something was, or was not a Hotdog.\n",
        "\n",
        "As an example, let's implement this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vqm5X7k-tPRR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEO1SjISvJmk"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset that we will be using already segmented our images to train and test. (How convenient!)\n",
        "\n",
        "https://www.kaggle.com/datasets/dansbecker/hot-dog-not-hot-dog/data\n",
        "\n",
        "Let's get started by defining our dataloader and dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nphc7iG7wfVV",
        "outputId": "84ffe0ce-876d-4b18-de91-b420c2378f04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hotdog-not-hotdog-dataset'...\n",
            "remote: Enumerating objects: 4586, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 4586 (delta 0), reused 1 (delta 0), pack-reused 4583 (from 1)\u001b[K\n",
            "Receiving objects: 100% (4586/4586), 223.14 MiB | 14.32 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "Updating files: 100% (4905/4905), done.\n"
          ]
        }
      ],
      "source": [
        "# we need to download this dataset from github (this is on kaggle\n",
        "# but it is a longer process to download from kaggle; need api token)\n",
        "!git clone https://github.com/youngsoul/hotdog-not-hotdog-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZLSfgnxr6MI"
      },
      "outputs": [],
      "source": [
        "class HotdogDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Load image paths and labels\n",
        "        for label, sub_dir in enumerate(['hot_dog', 'not_hot_dog']):\n",
        "            sub_dir_path = os.path.join(root_dir, sub_dir)\n",
        "            for filename in os.listdir(sub_dir_path):\n",
        "                if filename.endswith(\".jpg\"):\n",
        "                    self.image_paths.append(os.path.join(sub_dir_path, filename))\n",
        "                    self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Loads and returns a sample (image and label).\"\"\"\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thxwRi0dy7DS"
      },
      "source": [
        "### Define our Transforms & Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeGz968wy15C"
      },
      "outputs": [],
      "source": [
        "# the transforms that we are using\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resize to a fixed size\n",
        "    transforms.ToTensor(),  # Convert image to a tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
        "])\n",
        "\n",
        "train_dataset = HotdogDataset(\"./hotdog-not-hotdog-dataset/train\", transform=transform)\n",
        "test_dataset = HotdogDataset(\"./hotdog-not-hotdog-dataset/test\", transform=transform)\n",
        "val_dataset = HotdogDataset(\"./hotdog-not-hotdog-dataset/holdout\", transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge7zX-HDzA72"
      },
      "source": [
        "### Create our Run\n",
        "\n",
        "For now, you can just follow the instructions to login, however, if you are interested in secrets management, see the following:\n",
        "\n",
        "```python\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "    from google.colab import userdata\n",
        "    key = userdata.get('wandb_api_key')\n",
        "elif 'KAGGLE_CONTAINER_NAME' in os.environ:\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "    user_secrets = UserSecretsClient()\n",
        "    key = user_secrets.get_secret(\"wandb_api_key\")\n",
        "else:\n",
        "    key = None\n",
        "\n",
        "wandb.login(key=key)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "kL4TezRpzB0A",
        "outputId": "4fb682bc-7af3-475f-b1d7-749e6a0ce641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manony-moose-183990734713950594\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250401_141122-2hazkmqk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/anony-moose-183990734713950594/uncategorized/runs/2hazkmqk?apiKey=bad82471dfc266d7b5db699c1b3a1a4c5f4419ab' target=\"_blank\">devout-music-1</a></strong> to <a href='https://wandb.ai/anony-moose-183990734713950594/uncategorized?apiKey=bad82471dfc266d7b5db699c1b3a1a4c5f4419ab' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/anony-moose-183990734713950594/uncategorized?apiKey=bad82471dfc266d7b5db699c1b3a1a4c5f4419ab' target=\"_blank\">https://wandb.ai/anony-moose-183990734713950594/uncategorized?apiKey=bad82471dfc266d7b5db699c1b3a1a4c5f4419ab</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/anony-moose-183990734713950594/uncategorized/runs/2hazkmqk?apiKey=bad82471dfc266d7b5db699c1b3a1a4c5f4419ab' target=\"_blank\">https://wandb.ai/anony-moose-183990734713950594/uncategorized/runs/2hazkmqk?apiKey=bad82471dfc266d7b5db699c1b3a1a4c5f4419ab</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Do NOT share these links with anyone. They can be used to claim your runs."
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "# define our run\n",
        "run = wandb.init(\n",
        "  project=\"hotdog-not-hotdog\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p8-0TxszggM"
      },
      "source": [
        "### Define our Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icVvVP4ozimT"
      },
      "outputs": [],
      "source": [
        "# define our configuration\n",
        "# feel free to change this\n",
        "run.config.update({\n",
        "    \"n_epochs\": 10,\n",
        "    \"lr\": 0.001,\n",
        "    \"batch_size\": 64,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFT7C-e9y9ap"
      },
      "source": [
        "### Create Dataloaders\n",
        "\n",
        "Create loaders for our three different datasets: train, test and holdout (validation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyZXi16Gy1z-"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=run.config[\"batch_size\"], shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=run.config[\"batch_size\"], shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=run.config[\"batch_size\"], shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaZD447J0kYv"
      },
      "source": [
        "### Create our Model\n",
        "\n",
        "We aren't really creating a model, just fine tuning one. This will save us some time ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PraMUEs0dxd"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\n",
        "    \"cuda\" if torch.cuda.is_available() else          # for GPUs\n",
        "    \"mps\" if torch.backends.mps.is_available() else   # for Apple Silicon chips\n",
        "    \"cpu\"                                             # else\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4hKFasm1A3C",
        "outputId": "13e680b7-3f74-44d8-dd59-c5919d2ca215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 124MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models import resnet50\n",
        "\n",
        "# define our model, optimizer, and criterion\n",
        "model = resnet50(weights=\"IMAGENET1K_V2\").to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=run.config[\"lr\"])\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# todo: log this information into the run configuration"
      ],
      "metadata": {
        "id": "lXCARo8jCgDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuXd4m9m3UDq"
      },
      "source": [
        "### Prepare for Stats\n",
        "\n",
        "Open the dashboard (either in another tab, or here!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "U2W0lnsM3XLm",
        "outputId": "9b240b12-0511-4662-e95b-082ae3c8931a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<wandb.jupyter.IFrame at 0x7c370954c750>"
            ],
            "text/html": [
              "<iframe src='https://wandb.ai/anony-moose-183990734713950594/uncategorized/runs/2hazkmqk?apiKey=bad82471dfc266d7b5db699c1b3a1a4c5f4419ab?jupyter=true' style='border:none;width:100%;height:420px;'></iframe>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# you can only see this if you are logged in.\n",
        "%wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm9ZPB9l1DWh"
      },
      "source": [
        "### Train our Model\n",
        "\n",
        "Let's create our training loop..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxOzPDMS1GEJ"
      },
      "outputs": [],
      "source": [
        "for epoch in range(run.config[\"n_epochs\"]):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    running_loss = 0\n",
        "\n",
        "    model.train()  # set train model\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # fwd pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # backwards pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # tracking\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # calculate more stats\n",
        "    avg_loss = running_loss / len(train_dataset)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    model.eval()\n",
        "    total_val = 0\n",
        "    correct_val = 0\n",
        "    running_loss_val = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # tracking\n",
        "            running_loss_val += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss_val = running_loss_val / len(val_loader)\n",
        "    accuracy_val = correct_val / total_val\n",
        "\n",
        "    # todo: add more statistics here\n",
        "    run.log({\n",
        "        \"epoch\": epoch + 1,                                       # general tracking\n",
        "        \"loss\": avg_loss, \"accuracy\": accuracy,                   # training loop\n",
        "        \"val_loss\": avg_loss_val, \"val_accuracy\": accuracy_val    # val loop\n",
        "    })"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}