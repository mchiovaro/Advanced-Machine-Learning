{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a5c6062d",
      "metadata": {
        "id": "a5c6062d"
      },
      "source": [
        "# **Interpreting ML Models with LIME: A Hands-on Workshop**\n",
        "In this workshop, we'll explore how to use Local Interpretable Model-Agnostic Explanations (LIME) to interpret machine learning models, both for tabular and image data. This workshop was created for ELE392, taught by Dr. Chiovaro at the University of Rhode Island\n",
        "\n",
        "**Topics Covered:**\n",
        "\n",
        "*  Using LIME for Tabular Classification\n",
        "*  Using LIME for Image Explanation\n",
        "*  Using LIME for Text Explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Switch to GPU first if available.**"
      ],
      "metadata": {
        "id": "Zm2OV5UwiLbW"
      },
      "id": "Zm2OV5UwiLbW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c19485e1",
      "metadata": {
        "id": "c19485e1"
      },
      "outputs": [],
      "source": [
        "# !pip install lime pandas tqdm scikit-learn matplotlib Pillow torch torchvision transformers datasets\n",
        "!pip install lime tqdm Pillow transformers datasets\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm #Ben found this\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from lime import lime_tabular, lime_image\n",
        "from PIL import Image\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as T\n",
        "from skimage.segmentation import mark_boundaries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a54102c",
      "metadata": {
        "id": "8a54102c"
      },
      "source": [
        "## **Part 1: LIME for Tabular Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Training Model on IRIS Dataset:**\n",
        "\n",
        "We will use logistic regression to classify the Iris dataset and use LIME to interpret the predictions.\n",
        "\n",
        "The model implementation and training is the same as any other lab we have previously done."
      ],
      "metadata": {
        "id": "SvgvPMrODJBa"
      },
      "id": "SvgvPMrODJBa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "060dac26",
      "metadata": {
        "id": "060dac26"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "df['target_name'] = df['target'].apply(lambda i: iris.target_names[i])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1da06864",
      "metadata": {
        "id": "1da06864"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[iris.feature_names].values\n",
        "y = df['target'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dc78ba4",
      "metadata": {
        "id": "6dc78ba4"
      },
      "outputs": [],
      "source": [
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "model = nn.Linear(X_train.shape[1], len(iris.target_names))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "model.train()\n",
        "for epoch in tqdm(range(200), desc='Training'):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_t)\n",
        "    loss = criterion(outputs, y_train_t)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "195a0246",
      "metadata": {
        "id": "195a0246"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
        "with torch.no_grad():\n",
        "    logits = model(X_test_t)\n",
        "    preds = logits.argmax(dim=1).numpy()\n",
        "    acc = (preds == y_test).mean()\n",
        "print(f'Test Accuracy: {acc * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Using LIME to Explain Model:**\n",
        "\n",
        "After you have a trained model, LIME can be implemented to explain its predictions. This will be done in 3 main parts.\n"
      ],
      "metadata": {
        "id": "l24cT7C5DPQl"
      },
      "id": "l24cT7C5DPQl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "####**1) Create Explainer:**\n",
        "\n",
        "This section initializes the explainer using LimeTabularExplainer()\n",
        "\n",
        "There are 4 parameters needed.\n",
        "\n",
        "\n",
        "\n",
        "1.   **training_data**\n",
        "\n",
        "  The 2d numpy array of the training data used by the model.\n",
        "\n",
        "2.   **mode**\n",
        "\n",
        "  Either 'classification' or 'regression'\n",
        "\n",
        "3.   **feature_names**\n",
        "\n",
        "  List of names (strings) of the features\n",
        "\n",
        "4.   **class_names**\n",
        "\n",
        "  List of names of the classes (targets)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vr2OdeiWO8cY"
      },
      "id": "Vr2OdeiWO8cY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**2) Define Prediction Function:**\n",
        "\n",
        "  For *classifiers,* this should be a function that takes a numpy array and outputs prediction probabilities.\n",
        "\n",
        "  For *regressors,* this takes a numpy array and returns the predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "ulsV81UVD5ZN"
      },
      "id": "ulsV81UVD5ZN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**3) Create Explanation:**\n",
        "\n",
        "Call explain_instance() from the explainer.\n",
        "\n",
        "This uses the following parameters:\n",
        "\n",
        "1.  **data_row**\n",
        "\n",
        "  1d numpy array corresponding to a row of input data.\n",
        "\n",
        "2.  **predict_fn**\n",
        "\n",
        "  Prediction function.\n",
        "\n",
        "3.  **num_features**\n",
        "\n",
        "  The maximum number of features present in the explanation.\n",
        "\n",
        "There are multiple ways to display the explanation including:\n",
        "\n",
        "*  as_html()\n",
        "*  as_list()\n",
        "*  as_map()\n",
        "*  as_pyplot_figure()\n",
        "* show_in_notebook()\n",
        "\n",
        "In Colab, show_in_notebook() is the best way to view the explanations.\n",
        "\n",
        "Additionally, the html explanation can be downloaded using save_to_file()"
      ],
      "metadata": {
        "id": "lMdmDgEsD66P"
      },
      "id": "lMdmDgEsD66P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a1ec73e",
      "metadata": {
        "id": "0a1ec73e"
      },
      "outputs": [],
      "source": [
        "# Create Explainer\n",
        "explainer = lime_tabular.LimeTabularExplainer(\n",
        "    X_train,\n",
        "    feature_names=iris.feature_names,\n",
        "    class_names=iris.target_names.tolist(),\n",
        "    mode='classification')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Prediction Function\n",
        "def predict_fn(input):\n",
        "    input_t = torch.tensor(input, dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_t)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "    return probs.numpy()"
      ],
      "metadata": {
        "id": "pdv1nw9wapkS"
      },
      "id": "pdv1nw9wapkS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Explanation\n",
        "i = 0\n",
        "exp = explainer.explain_instance(X_test[i], predict_fn, num_features=4)\n",
        "exp.show_in_notebook(show_table=True)"
      ],
      "metadata": {
        "id": "WuVYXKnpapU6"
      },
      "id": "WuVYXKnpapU6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1a9711a2",
      "metadata": {
        "id": "1a9711a2"
      },
      "source": [
        "## **Part 2: LIME for Image Classification**\n",
        "We now use a pretrained ResNet50 model and LIME to highlight important regions in an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63a8c0f6",
      "metadata": {
        "id": "63a8c0f6"
      },
      "outputs": [],
      "source": [
        "# Get Image\n",
        "!wget -O cat.jpg https://upload.wikimedia.org/wikipedia/commons/thumb/b/b6/Felis_catus-cat_on_snow.jpg/640px-Felis_catus-cat_on_snow.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdf703d1",
      "metadata": {
        "id": "fdf703d1"
      },
      "outputs": [],
      "source": [
        "# Prep model and image\n",
        "\n",
        "model_cnn = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "model_cnn.eval()\n",
        "\n",
        "transform = T.Compose([\n",
        "    T.Resize(224),\n",
        "    T.CenterCrop(224),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "img = Image.open('cat.jpg')\n",
        "plt.imshow(img); plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Using LIME to Explain Image:**\n",
        "\n",
        "LIME can be used to explain the most important features of an image when using a classification model. Similar to the previous example, there are 3 main parts:"
      ],
      "metadata": {
        "id": "UrYxSLLwDvm9"
      },
      "id": "UrYxSLLwDvm9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**1) Define Classifier Function:**\n",
        "\n",
        " This function takes a numpy array and outputs prediction probabilities.\n",
        "\n"
      ],
      "metadata": {
        "id": "VSP3Daelniok"
      },
      "id": "VSP3Daelniok"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**2) Create Explainer:**\n",
        "\n",
        "This section initializes the explainer using LimeImageExplainer()\n",
        "\n",
        "The explain_instance() function is called. For this, here are 5 parameters needed.\n",
        "\n",
        "\n",
        "\n",
        "1.   **image**\n",
        "\n",
        "  The numpy array of the image.\n",
        "\n",
        "2.   **classifier_fn**\n",
        "\n",
        "  The classifier function defined above.\n",
        "\n",
        "3.   **top_labels**\n",
        "\n",
        "  Produce explanations for the n labels with highest prediction probabilities, where n is this parameter.\n",
        "\n",
        "4.   **hide_color**\n",
        "\n",
        "  The color in which the masked pixels are set to. (None or 0 to 255)\n",
        "\n",
        "5.  **num_samples**\n",
        "\n",
        "  Size of the neighborhood to learn the linear model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mw9T0hR4EJQ3"
      },
      "id": "mw9T0hR4EJQ3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**3) Create Explanation:**\n",
        "\n",
        "The get_image_and_mask() function will return the masked image created. 4 parameters are needed:\n",
        "\n",
        "1.  **label**\n",
        "\n",
        "  Label to explain\n",
        "\n",
        "2.  **positive_only**\n",
        "\n",
        "  If True, only take superpixels that have positively contributed to the prediction.\n",
        "  \n",
        "  (The negative_only parameter can be used to highlight the pixels that have negatively contributed)\n",
        "\n",
        "3. **hide_rest**\n",
        "\n",
        "  If true, make the non-explaining parts of the image gray.\n",
        "\n",
        "4. **num_features**\n",
        "\n",
        "  Number of superpixels to include in the explaination"
      ],
      "metadata": {
        "id": "ajrz_9eLEKlJ"
      },
      "id": "ajrz_9eLEKlJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2de659f",
      "metadata": {
        "id": "e2de659f"
      },
      "outputs": [],
      "source": [
        "# Image Prediction Function\n",
        "def predict_img(images):\n",
        "    batch = torch.stack([transform(Image.fromarray(img)).to('cpu') for img in images])\n",
        "    with torch.no_grad():\n",
        "        logits = model_cnn(batch)\n",
        "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "    return probs.cpu().numpy()\n",
        "\n",
        "# Explain the image\n",
        "image_explainer = lime_image.LimeImageExplainer()\n",
        "explanation = image_explainer.explain_instance(\n",
        "    np.array(img),\n",
        "    predict_img,\n",
        "    top_labels=1,\n",
        "    hide_color=0,\n",
        "    num_samples=1000)\n",
        "\n",
        "# Get the masked image\n",
        "temp, mask = explanation.get_image_and_mask(\n",
        "    explanation.top_labels[0],\n",
        "    positive_only=True,\n",
        "    hide_rest=True,\n",
        "    num_features=5)\n",
        "\n",
        "# Show the image\n",
        "plt.imshow(mark_boundaries(temp / 255.0, mask)); plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 3: LIME for Text Classification**"
      ],
      "metadata": {
        "id": "irJ2488Ac1MV"
      },
      "id": "irJ2488Ac1MV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Importing Pre-Trained Model:**\n",
        "\n",
        "We now use a pretrained RoBERTa model on the IMDB movie reviews dataset."
      ],
      "metadata": {
        "id": "UUn4uOlcjR2m"
      },
      "id": "UUn4uOlcjR2m"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, get_scheduler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "wtskjQEzdx44"
      },
      "id": "wtskjQEzdx44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the IMDB dataset (already pre-split into train and test sets)\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Reduce dataset size to save memory (use only 500 training samples)\n",
        "train_texts = dataset['train']['text'][:500]\n",
        "train_labels = dataset['train']['label'][:500]\n",
        "\n",
        "# Use full test set for evaluation\n",
        "test_texts = dataset['test']['text'][:100]\n",
        "test_labels = dataset['test']['label'][:100]\n",
        "\n",
        "# Print dataset size\n",
        "print(f\"Training samples: {len(train_texts)}\")\n",
        "print(f\"Testing samples: {len(test_texts)}\")"
      ],
      "metadata": {
        "id": "uq5T456_e2Th"
      },
      "id": "uq5T456_e2Th",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"roberta-base\"\n",
        "\n",
        "# Load the tokenizer for DistilBERT (a smaller version of BERT)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Function to tokenize text\n",
        "def tokenize_function(texts):\n",
        "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
        "\n",
        "# Tokenize training and testing texts\n",
        "train_encodings = tokenize_function(train_texts)\n",
        "test_encodings = tokenize_function(test_texts)\n",
        "\n",
        "# Convert labels into PyTorch tensors\n",
        "train_labels_tensor = torch.tensor(train_labels)\n",
        "test_labels_tensor = torch.tensor(test_labels)"
      ],
      "metadata": {
        "id": "8xUy00ZDfFbS"
      },
      "id": "8xUy00ZDfFbS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset class to handle tokenized data\n",
        "class IMDBDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings  # Tokenized text\n",
        "        self.labels = labels  # Corresponding labels (0 = negative, 1 = positive)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)  # Number of samples in dataset\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieve tokenized text and label for a given index\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "# Create dataset objects for training and testing\n",
        "train_dataset = IMDBDataset(train_encodings, train_labels_tensor)\n",
        "test_dataset = IMDBDataset(test_encodings, test_labels_tensor)"
      ],
      "metadata": {
        "id": "lxnAbbGFfJRG"
      },
      "id": "lxnAbbGFfJRG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "_zBIYtRefMEV"
      },
      "id": "_zBIYtRefMEV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained DistilBERT model for binary classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# Move model to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "7FysgzqkfObW"
      },
      "id": "7FysgzqkfObW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model to evaluation mode (disables dropout and gradients)\n",
        "model.eval()\n",
        "\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "# Disable gradient computation to save memory during testing\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = {key: val.to(device) for key, val in batch.items()}  # Move batch to device\n",
        "        outputs = model(**batch)  # Forward pass\n",
        "        logits = outputs.logits  # Get model outputs\n",
        "\n",
        "        # Convert logits to class predictions (0 or 1)\n",
        "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        labels = batch[\"labels\"].cpu().numpy()\n",
        "\n",
        "        # Store predictions and actual labels for accuracy calculation\n",
        "        predictions.extend(preds)\n",
        "        true_labels.extend(labels)\n",
        "\n",
        "# Compute accuracy score\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "5x8Ji1nzfXCV"
      },
      "id": "5x8Ji1nzfXCV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Using LIME to Explain Text:**\n",
        "\n",
        "LIME can be used to explain the most important words when classifying text. Similar to the previous example, there are 3 main parts:"
      ],
      "metadata": {
        "id": "pVWNph4bj-at"
      },
      "id": "pVWNph4bj-at"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**1) Define Classifier Function:**\n",
        "\n",
        "  This function takes a list of d strings and outputs a (d, k) numpy array with prediction probabilities, where k is the number of classes."
      ],
      "metadata": {
        "id": "XHjpHGfAkUwL"
      },
      "id": "XHjpHGfAkUwL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**2) Create Explainer**\n",
        "\n",
        "  The LimeTextExplainer() will use one parameter:\n",
        "\n",
        "  *  **class_names**\n",
        "  \n",
        "  list of class names, ordered according to whatever the classifier is using. If not present, class names will be ‘0’, ‘1’, …"
      ],
      "metadata": {
        "id": "9S0HQe9WksCZ"
      },
      "id": "9S0HQe9WksCZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**3) Create Explanation**\n",
        "\n",
        "  Call explain_instance() from the explainer. It will use the following parameters:\n",
        "\n",
        "  * **text_instance**\n",
        "\n",
        "  Raw text string to be explained.\n",
        "  \n",
        "  * **classifier_fn**\n",
        "\n",
        "  Classifier function defined above.\n",
        "\n",
        "  * **num_features**\n",
        "\n",
        "  The number of words to be explained in the text."
      ],
      "metadata": {
        "id": "IFcUhRgOlFWp"
      },
      "id": "IFcUhRgOlFWp"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "# Create a Hugging Face pipeline for classification\n",
        "# This is a way to simplify the text classification process.\n",
        "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, return_all_scores=True, device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "def predict_proba(texts):\n",
        "    predictions = classifier(texts)\n",
        "    return np.array([[p[\"score\"] for p in pred] for pred in predictions])\n"
      ],
      "metadata": {
        "id": "zWetNiDRfbNy"
      },
      "id": "zWetNiDRfbNy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create LIME Explainer\n",
        "explainer = LimeTextExplainer(class_names=[\"negative\", \"positive\"])\n",
        "\n",
        "# Pick a sample\n",
        "idx = 0\n",
        "sample_text = test_texts[idx]\n",
        "true_label = test_labels[idx]\n",
        "\n",
        "# Create explanation\n",
        "explanation = explainer.explain_instance(\n",
        "    sample_text,\n",
        "    predict_proba,  # Function to get model probabilities\n",
        "    num_features=10  # Number of words to explain\n",
        ")\n",
        "\n",
        "# Show results\n",
        "print(f\"True Label: {'positive' if true_label == 1 else 'negative'}\")\n",
        "explanation.show_in_notebook()\n"
      ],
      "metadata": {
        "id": "i-0chYuqgQ8k"
      },
      "id": "i-0chYuqgQ8k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 4: Import Your Own Model**\n",
        "\n",
        "Use the code below to import one of your own pre-trained model weights and use LIME. You will also need to import the dataset that this model was trained on.\n",
        "If your dataset does not fit into one of the above examples, there is still likely a way to use LIME on your model. [See the documentation to find out how to implement it.](https://lime-ml.readthedocs.io/en/latest/lime.html)"
      ],
      "metadata": {
        "id": "LYEin-5iQR0Z"
      },
      "id": "LYEin-5iQR0Z"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import model weights\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Upload a file\n",
        "uploaded = files.upload()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Initialize your model.\n",
        "model = XXX.to(device)\n",
        "\n",
        "# Load the weights into the model\n",
        "model.load_state_dict(torch.load('model.pth'))"
      ],
      "metadata": {
        "id": "fuq70-eMQkhI"
      },
      "id": "fuq70-eMQkhI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset and get predictions using pre-trained model"
      ],
      "metadata": {
        "id": "7ZrRtZUcRN1X"
      },
      "id": "7ZrRtZUcRN1X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5cca78ff",
      "metadata": {
        "id": "5cca78ff"
      },
      "source": [
        "## Conclusion\n",
        "You’ve now seen how to use LIME to explain predictions for tabular, text, and image-based models. LIME is a powerful tool to gain insight into the decision-making of your machine learning models!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}