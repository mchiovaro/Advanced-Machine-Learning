{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9a0154b4",
      "metadata": {
        "id": "9a0154b4"
      },
      "source": [
        "# Lab 6: Optimization Methods in PyTorch\n",
        "\n",
        "The goal of this lab is to improve the performance of a deep learning model by implementing various regularization and normalization techniques in PyTorch.\n",
        "\n",
        "**What You'll Do:**\n",
        "- Apply **L1/L2 weight decay**.\n",
        "- Implement **Dropout**.\n",
        "- Implement **Normalization (BatchNorm, LayerNorm, etc.)**.\n",
        "- Use **Early Stopping**.\n",
        "- Experiment with **Data Augmentation (CutMix, Mixup)**.\n",
        "\n",
        "You'll be given challenges where you must use the **PyTorch documentation** to complete missing parts!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Imports"
      ],
      "metadata": {
        "id": "_NDtx4jouESj"
      },
      "id": "_NDtx4jouESj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24a86b48",
      "metadata": {
        "id": "24a86b48"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(24)  # For reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5211f047",
      "metadata": {
        "id": "5211f047"
      },
      "source": [
        "## Part 2: Load the Dataset\n",
        "We'll use CIFAR-10, a small image classification dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "808d41a5",
      "metadata": {
        "id": "808d41a5"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "val_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b42e443",
      "metadata": {
        "id": "5b42e443"
      },
      "source": [
        "## Part 3. Define a Simple CNN\n",
        "We'll start with a basic CNN model and optimize it throughout the challenge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4294419",
      "metadata": {
        "id": "e4294419"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32 * 16 * 16, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdd20689",
      "metadata": {
        "id": "fdd20689"
      },
      "source": [
        "## Part 5: Apply L2 Regularization (Weight Decay)\n",
        "Modify the optimizer to use **L2 regularization**. Check out the PyTorch documentation for the Adam optimizer [here](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14719b44",
      "metadata": {
        "id": "14719b44"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3f0341c",
      "metadata": {
        "id": "b3f0341c"
      },
      "source": [
        "## Part 6: Add Dropout to the Model\n",
        "Modify the **SimpleCNN** class you created above to include **Dropout layers**. Check out the documentation on dropout layers [here](https://pytorch.org/docs/stable/nn.html#dropout-layers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4507ef20",
      "metadata": {
        "id": "4507ef20"
      },
      "outputs": [],
      "source": [
        "class DropoutCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DropoutCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(p=.2)\n",
        "        self.fc1 = nn.Linear(32 * 16 * 16, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "model = DropoutCNN()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7659075a",
      "metadata": {
        "id": "7659075a"
      },
      "source": [
        "## Part 7: Add Batch Normalization\n",
        "Modify the CNN to include **BatchNorm** after each convolutional layer.\n",
        "\n",
        "**Extra challenge**: Try to find the documentation for this on your own!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91e0b952",
      "metadata": {
        "id": "91e0b952"
      },
      "outputs": [],
      "source": [
        "class BatchNormCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BatchNormCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32 * 16 * 16, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.batchnorm1(self.conv1(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "model = BatchNormCNN()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b80e7cf",
      "metadata": {
        "id": "9b80e7cf"
      },
      "source": [
        "## Part 8: Implement Early Stopping\n",
        "**Challenge 1**: Write the training code into the loop\n",
        "\n",
        "**Challenge 2**: Modify the loop to stop if validation loss doesn't improve after **N epochs**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "869965f4",
      "metadata": {
        "id": "869965f4"
      },
      "outputs": [],
      "source": [
        "best_loss = float('inf')\n",
        "patience = 5  # Adjust as needed\n",
        "counter = 0\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs, targets = batch\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)  # Compute average training loss\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            inputs, targets = batch\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)  # Compute average validation loss\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Validation Loss = {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print('Early stopping triggered!')\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 9: Data Augmentation\n",
        "\n",
        "**Here's the big challenge for today**: Implement both ***CutMix*** and ***MixUp*** to your data!\n",
        "\n",
        "1.   Augment your data with both CutMix and MixUp. In other words, randomly select images and apply one or the other method. Do not apply both methods to the same image.\n",
        "2.   Use your new augmented data to train a simple CNN with Dropout, BatchNorm, and Early Stopping.\n",
        "\n"
      ],
      "metadata": {
        "id": "f5yYf9z1yD1Z"
      },
      "id": "f5yYf9z1yD1Z"
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import v2\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "cutmix = v2.CutMix(num_classes=10)\n",
        "mixup = v2.MixUp(num_classes=10)\n",
        "cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])\n",
        "\n",
        "# Initialize for early stopping\n",
        "best_val_loss = float('inf')\n",
        "patience = 5  # Number of epochs to wait for improvement\n",
        "counter = 0\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = cutmix_or_mixup(images, labels)  # Apply CutMix or MixUp\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)  # Average training loss\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)  # Average validation loss\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Validation Loss = {val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0  # Reset counter if validation loss improves\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break  # Stop training"
      ],
      "metadata": {
        "id": "rm_fYNHyrjh6"
      },
      "id": "rm_fYNHyrjh6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Say you want to only apply it to a proportion of your data.**\n",
        "For each batch, you could generate a random number r (between 0 and 1) such that:\n",
        "\n",
        "* If `r < mixup_prob`, MixUp is applied.\n",
        "* If `r < mixup_prob + cutmix_prob`, CutMix is applied.\n",
        "* Otherwise, no augmentation is applied."
      ],
      "metadata": {
        "id": "AFwJxvLbsm9k"
      },
      "id": "AFwJxvLbsm9k"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}